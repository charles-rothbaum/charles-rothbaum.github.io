<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Visualizer</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- External libraries -->
  <script src="https://unpkg.com/lodash"></script>
  <script src="https://unpkg.com/butterchurn"></script>
  <script src="https://unpkg.com/butterchurn-presets"></script>
  <script src="https://unpkg.com/butterchurn-presets/lib/butterchurnPresetsExtra.min.js"></script>
  <script src="https://code.jquery.com/jquery-3.1.1.min.js"
          integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8="
          crossorigin="anonymous"></script>
  
  <link rel="stylesheet" href="https://unpkg.com/normalize.css/normalize.css" />
  
  <style>
  </style>
</head>
<body>
  <div id="mainWrapper">
    <div id="audioSelectWrapper"></div>
    <canvas id="canvas" width="640" height="300"></canvas>
    <div id="micSelect"><span>Use Mic</span></div>
    <div id="loadPresetBut"><span>Load local preset</span></div>
  </div>

  <script>
    $(function () {
      'use strict';

      // DOM elements
      const canvas = document.getElementById('canvas');
      const $audioSelectWrapper = $('#audioSelectWrapper');
      const $micSelect = $('#micSelect');
      const $loadPresetBut = $('#loadPresetBut');

      // Global state variables
      let visualizer = null;
      let rendering = false;
      let audioContext = null;
      let sourceNode = null;
      let delayedAudible = null;
      let file = null;

      // Initialize the audio context and Butterchurn visualizer
      const initPlayer = () => {
        audioContext = new AudioContext();
        visualizer = butterchurn.default.createVisualizer(audioContext, canvas, {
          width: 640,
          height: 300,
          textureRatio: 1.2,
        });
      };

      // Connect an audio node to the visualizer with a delay effect
      const connectToAudioAnalyzer = (node) => {
        if (delayedAudible) {
          delayedAudible.disconnect();
        }
        delayedAudible = audioContext.createDelay();
        delayedAudible.delayTime.value = 0.26;
        node.connect(delayedAudible);
        delayedAudible.connect(audioContext.destination);
        visualizer.connectAudio(delayedAudible);
      };

      // Start the render loop for the visualizer
      const startRenderer = () => {
        const renderLoop = () => {
          visualizer.render();

          requestAnimationFrame(renderLoop);
        };
        requestAnimationFrame(renderLoop);
      };

      // Play an audio buffer using the visualizer
      const playBufferSource = (buffer) => {
        if (!rendering) {
          rendering = true;
          startRenderer();
        }
        if (sourceNode) {
          sourceNode.disconnect();
        }
        sourceNode = audioContext.createBufferSource();
        sourceNode.buffer = buffer;
        connectToAudioAnalyzer(sourceNode);
        sourceNode.start(0);
      };

      // Load and play local audio files sequentially
      const loadLocalFiles = (files, index = 0) => {
        audioContext.resume();
        const reader = new FileReader();
        reader.onload = (event) => {
          audioContext.decodeAudioData(event.target.result, (buffer) => {
            playBufferSource(buffer);
            setTimeout(() => {
              if (files.length > index + 1) {
                loadLocalFiles(files, index + 1);
              } else {
                if (sourceNode) {
                  sourceNode.disconnect();
                  sourceNode = null;
                }
                $audioSelectWrapper.css('display', 'block');
              }
            }, buffer.duration * 1000);
          });
        };
        const currentFile = files[index];
        reader.readAsArrayBuffer(currentFile);
      };

      // Connect microphone input to the visualizer
      const connectMicAudio = (stream) => {
        audioContext.resume();
        const micSourceNode = audioContext.createMediaStreamSource(stream);
        const gainNode = audioContext.createGain();
        gainNode.gain.value = 1.25;
        micSourceNode.connect(gainNode);
        visualizer.connectAudio(gainNode);
        startRenderer();
      };

      // Load a preset from a local file and apply it to the visualizer
      $loadPresetBut.click(() => {
        const fileSelector = $('<input type="file" accept=".json">');
        fileSelector.on('change', (event) => {
          file = event.target.files[0];
          if (!file) return;
          const reader = new FileReader();
          reader.onload = (event) => {
            try {
              const preset = JSON.parse(event.target.result);
              visualizer.loadPreset(preset, 5.7);
            } catch (err) {
              console.error("Error parsing preset file:", err);
            }
          };
          reader.readAsText(file);
        });
        fileSelector.click();
      });

      // Handle microphone selection and audio input
      $micSelect.click(() => {
        if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
          navigator.mediaDevices.getUserMedia({ audio: true })
            .then((stream) => connectMicAudio(stream))
            .catch((err) => {
              console.error('Error getting audio stream from getUserMedia:', err);
            });
        } else {
          console.error('getUserMedia is not supported in this browser.');
        }
      });

      // Initialize the visualizer on page load
      initPlayer();
      canvas.width = 640;
      canvas.height = 300;
    });
  </script>
</body>
</html>
